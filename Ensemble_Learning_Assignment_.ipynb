{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1: What is Ensemble Learning in machine learning? Explain the key idea behind it.**\n",
        "\n",
        "**Answer:**It is a machine learning technique in which multiple models (often called base learners or weak learners) are trained and then combined to solve a problem. Instead of relying on a single model, ensemble methods aggregate the predictions of multiple models to achieve better performance, accuracy, and robustness.\n",
        "\n",
        "The key idea behind ensemble learning is the principle of the “wisdom of the crowd”. Just like a group of people with diverse perspectives can collectively make better decisions than a single person, combining multiple models reduces the risk of individual errors and improves generalization.\n",
        "\n",
        "* If one model makes a mistake, others may correct it.\n",
        "* Different models may capture different aspects of the data.\n",
        "* The combined prediction is usually more stable and less prone to overfitting compared to individual models."
      ],
      "metadata": {
        "id": "e5egx6WbqFd4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2: What is the difference between Bagging and Boosting?**\n",
        "\n",
        "**Answer:**Bagging and Boosting are both ensemble learning techniques, but they differ in how they build and combine models:\n",
        "\n",
        "Bagging (Bootstrap Aggregating):\n",
        "\n",
        "* Models are trained independently on different random subsets of the training data (sampled with replacement).\n",
        "* The final prediction is made by averaging (for regression) or majority voting (for classification).\n",
        "* Goal: Reduce variance and prevent overfitting.\n",
        "* Example: Random Forest.\n",
        "\n",
        "Boosting:\n",
        "\n",
        "* Models are trained sequentially, where each new model focuses on correcting the errors made by the previous models.\n",
        "* The final prediction is a weighted combination of all models.\n",
        "* Goal: Reduce bias and improve accuracy.\n",
        "*nExamples: AdaBoost, Gradient Boosting, XGBoost.\n",
        "\n"
      ],
      "metadata": {
        "id": "B11F5S1Xq5OC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3: What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?**\n",
        "\n",
        "**Answer:**It is a statistical technique where we create multiple random samples from the original dataset with replacement.\n",
        "\n",
        "* Each bootstrap sample has the same size as the original dataset, but because sampling is done with replacement, some data points may appear multiple times, while others may not appear at all.\n",
        "\n",
        "Role in Bagging (e.g., Random Forest):\n",
        "\n",
        "* In Bagging, each base learner (e.g., decision tree) is trained on a different bootstrap sample.\n",
        "* This introduces diversity among the models because each one sees a slightly different version of the training data.\n",
        "* When their predictions are combined (via averaging or majority vote), the overall model becomes more robust, stable, and less prone to overfitting compared to a single model trained on the full dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "xlhQRf-irkqH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4:What are Out-of-Bag (OOB) samples and how is OOB score used to\n",
        "evaluate ensemble models?**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "When using bootstrap sampling in Bagging (like in Random Forests), each bootstrap sample is created by sampling with replacement. On average, about 63% of the original training data points appear in a bootstrap sample, leaving the remaining  37% unused.\n",
        "\n",
        "* These unused data points are called **Out-of-Bag (OOB) samples**.\n",
        "\n",
        "**Role of OOB Samples:**\n",
        "\n",
        "* OOB samples act like a **built-in validation set**.\n",
        "* For each model in the ensemble, its OOB samples can be used to test its performance since those samples were not seen during training.\n",
        "\n",
        "**OOB Score:**\n",
        "\n",
        "* The **OOB score** is the average accuracy (or other performance metric) computed by aggregating predictions on all OOB samples across the ensemble.\n",
        "* It provides an **unbiased estimate of model performance** without needing a separate validation or test dataset."
      ],
      "metadata": {
        "id": "_MYXnC8lsRTF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5: Compare feature importance analysis in a single Decision Tree vs. a Random Forest.**\n",
        "\n",
        "**Answer: **Feature importance tells us how much each feature contributes to the predictive power of the model.\n",
        "\n",
        "*In a Single Decision Tree:*\n",
        "\n",
        "* Feature importance is calculated based on how much a feature reduces impurity (e.g., Gini impurity, entropy, or variance) across all the nodes where it is used for splitting.\n",
        "* The importance score is biased toward features with more categories or continuous variables, since they provide more splitting opportunities.\n",
        "* Because it depends on only one tree, it may give unstable results (high variance) if the dataset is small or noisy.\n",
        "\n",
        "*In a Random Forest:*\n",
        "\n",
        "* Feature importance is computed by averaging impurity reduction across all trees in the forest.\n",
        "* This aggregation makes the importance scores more reliable, stable, and less biased compared to a single tree.\n",
        "* Randomness in feature selection (feature bagging) further reduces bias toward dominant features."
      ],
      "metadata": {
        "id": "AqhXkM2ytUki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Question 6: Write a Python program to:\n",
        "● Load the Breast Cancer dataset using\n",
        "sklearn.datasets.load_breast_cancer()\n",
        "● Train a Random Forest Classifier\n",
        "● Print the top 5 most important features based on feature importance scores.\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "Answer:\n",
        "'''\n",
        "\n",
        "# Import required libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "\n",
        "# 1. Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# 2. Train a Random Forest Classifier\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "clf.fit(X, y)\n",
        "\n",
        "# 3. Get feature importance scores\n",
        "importances = clf.feature_importances_\n",
        "\n",
        "# Create a DataFrame for better visualization\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    \"Feature\": feature_names,\n",
        "    \"Importance\": importances\n",
        "})\n",
        "\n",
        "# Sort features by importance (descending order)\n",
        "feature_importance_df = feature_importance_df.sort_values(by=\"Importance\", ascending=False)\n",
        "\n",
        "# 4. Print Top 5 Most Important Features\n",
        "print(\"Top 5 Important Features:\")\n",
        "print(feature_importance_df.head(5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FyCXRS4t6nP",
        "outputId": "269c9716-b40e-4d0e-be07-44693f8105e3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Important Features:\n",
            "                 Feature  Importance\n",
            "23            worst area    0.139357\n",
            "27  worst concave points    0.132225\n",
            "7    mean concave points    0.107046\n",
            "20          worst radius    0.082848\n",
            "22       worst perimeter    0.080850\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Question 7: Write a Python program to:\n",
        "● Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "● Evaluate its accuracy and compare with a single Decision Tree\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "Answer:\n",
        "'''\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# 2. Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Train a single Decision Tree\n",
        "dt_clf = DecisionTreeClassifier(random_state=42)\n",
        "dt_clf.fit(X_train, y_train)\n",
        "y_pred_dt = dt_clf.predict(X_test)\n",
        "dt_accuracy = accuracy_score(y_test, y_pred_dt)\n",
        "\n",
        "# 4. Train a Bagging Classifier with Decision Trees\n",
        "bag_clf = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=50,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "bag_clf.fit(X_train, y_train)\n",
        "y_pred_bag = bag_clf.predict(X_test)\n",
        "bag_accuracy = accuracy_score(y_test, y_pred_bag)\n",
        "\n",
        "# 5. Print accuracy comparison\n",
        "print(\"Accuracy of Single Decision Tree:\", dt_accuracy)\n",
        "print(\"Accuracy of Bagging Classifier:\", bag_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-wcvUQSufVE",
        "outputId": "c327da1a-2cb4-4d79-d26f-a065739a84ca"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Single Decision Tree: 1.0\n",
            "Accuracy of Bagging Classifier: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Question 8: Write a Python program to:\n",
        "● Train a Random Forest Classifier\n",
        "● Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "● Print the best parameters and final accuracy\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "Answer:\n",
        "'''\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# 2. Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Define Random Forest model\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# 4. Define parameter grid for tuning\n",
        "param_grid = {\n",
        "    \"n_estimators\": [50, 100, 150],   # number of trees\n",
        "    \"max_depth\": [None, 5, 10, 15]    # depth of trees\n",
        "}\n",
        "\n",
        "# 5. Perform GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=rf,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,                # 5-fold cross-validation\n",
        "    n_jobs=-1,\n",
        "    scoring=\"accuracy\"\n",
        ")\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# 6. Get best parameters and model\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# 7. Evaluate final accuracy on test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "final_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Final Accuracy on Test Set:\", final_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YcfAhQ0ivP9O",
        "outputId": "12d140d0-8a5b-475d-d1fa-6c2199fb2911"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 5, 'n_estimators': 150}\n",
            "Final Accuracy on Test Set: 0.9707602339181286\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Question 9: Write a Python program to:\n",
        "● Train a Bagging Regressor and a Random Forest Regressor on the California\n",
        "Housing dataset\n",
        "● Compare their Mean Squared Errors (MSE)\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "Answer:\n",
        "'''\n",
        "# Import required libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# 1. Load California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "# 2. Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Train a Bagging Regressor (with Decision Trees as base estimator)\n",
        "bag_reg = BaggingRegressor(\n",
        "    estimator=DecisionTreeRegressor(),\n",
        "    n_estimators=50,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "bag_reg.fit(X_train, y_train)\n",
        "y_pred_bag = bag_reg.predict(X_test)\n",
        "mse_bag = mean_squared_error(y_test, y_pred_bag)\n",
        "\n",
        "# 4. Train a Random Forest Regressor\n",
        "rf_reg = RandomForestRegressor(\n",
        "    n_estimators=50,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "rf_reg.fit(X_train, y_train)\n",
        "y_pred_rf = rf_reg.predict(X_test)\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "\n",
        "# 5. Print comparison\n",
        "print(\"Mean Squared Error (Bagging Regressor):\", mse_bag)\n",
        "print(\"Mean Squared Error (Random Forest Regressor):\", mse_rf)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqTT1EwHvjg0",
        "outputId": "db52d93c-0191-4019-fe1b-54c02bac88ba"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (Bagging Regressor): 0.25787382250585034\n",
            "Mean Squared Error (Random Forest Regressor): 0.25772464361712627\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10: You are working as a data scientist at a financial institution to predict loan default. You have access to customer demographic and transaction history data. You decide to use ensemble techniques to increase model performance.**\n",
        "\n",
        "Explain your step-by-step approach to:\n",
        "* Choose between Bagging or Boosting\n",
        "* Handle overfitting\n",
        "* Select base models\n",
        "* Evaluate performance using cross-validation\n",
        "* Justify how ensemble learning improves decision-making in this real-world\n",
        "context.\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "**Answer:**  \n",
        "*Step 1: Choose between Bagging or Boosting*\n",
        "\n",
        "* Bagging (e.g., Random Forest) is useful when the main problem is high variance (unstable models like Decision Trees).\n",
        "* Boosting (e.g., XGBoost, AdaBoost, Gradient Boosting) is useful when the main problem is high bias (weak learners underfitting).\n",
        "\n",
        "For loan default prediction (imbalanced, complex relationships), Boosting is often preferred because it sequentially improves on mistakes and usually achieves higher accuracy.\n",
        "\n",
        "*Step 2: Handle Overfitting*\n",
        "\n",
        "* Use cross-validation to tune hyperparameters (max_depth, n_estimators, learning_rate).\n",
        "* Apply regularization techniques (e.g., shrinkage/learning rate in Boosting, limiting depth of trees).\n",
        "* Use early stopping (for Gradient Boosting / XGBoost).\n",
        "\n",
        "*Step 3: Select Base Models*\n",
        "\n",
        "* Typically Decision Trees are chosen as base models because they are simple and work well in ensembles.\n",
        "* For bagging: DecisionTreeClassifier ->Random Forest.\n",
        "* For boosting: Shallow Decision Trees (stumps) -> Gradient Boosting / XGBoost.\n",
        "\n",
        "*Step 4: Evaluate Performance with Cross-Validation*\n",
        "* Use StratifiedKFold Cross-Validation (because dataset is imbalanced).\n",
        "* Evaluate using metrics beyond accuracy (e.g., Precision, Recall, F1-score, AUC).\n",
        "\n",
        "*Step 5: Justification in Real-World Context*\n",
        "\n",
        "* Ensemble models reduce errors by combining multiple weak learners.\n",
        "* In loan default prediction, wrong decisions can be costly.\n",
        "* Ensemble methods improve robustness, generalization, and decision confidence, helping the financial institution minimize risk while maximizing approval rates.\n",
        "\n",
        "    \n",
        "    "
      ],
      "metadata": {
        "id": "U0uqsO99wLC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Question 10: You are working as a data scientist at a financial institution to predict loan default.\n",
        "You have access to customer demographic and transaction history data.\n",
        "You decide to use ensemble techniques to increase model performance.\n",
        "Explain your step-by-step approach to:\n",
        "\n",
        "Choose between Bagging or Boosting\n",
        "Handle overfitting\n",
        "Select base models\n",
        "Evaluate performance using cross-validation\n",
        "Justify how ensemble learning improves decision-making in this real-world context.\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n",
        "'''\n",
        "# Import libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "\n",
        "# 1. Simulate dataset (binary classification: loan default 0/1)\n",
        "X, y = make_classification(\n",
        "    n_samples=5000, n_features=20, n_informative=10,\n",
        "    n_redundant=5, n_classes=2, weights=[0.7, 0.3], random_state=42\n",
        ")\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# 2. Define base models\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "gb = GradientBoostingClassifier(random_state=42)\n",
        "\n",
        "# 3. Hyperparameter tuning for Gradient Boosting (Boosting)\n",
        "param_grid = {\n",
        "    \"n_estimators\": [100, 200],\n",
        "    \"max_depth\": [3, 5],\n",
        "    \"learning_rate\": [0.05, 0.1]\n",
        "}\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=gb, param_grid=param_grid, cv=cv,\n",
        "    scoring=\"roc_auc\", n_jobs=-1\n",
        ")\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best model\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# 4. Evaluate model\n",
        "y_pred = best_model.predict(X_test)\n",
        "y_proba = best_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"Best Parameters (Boosting):\", grid_search.best_params_)\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"ROC-AUC Score:\", roc_auc_score(y_test, y_proba))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJpVRf4txx5B",
        "outputId": "89b7bfa2-33d2-4efd-c3a8-02d2dfc62e60"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters (Boosting): {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.98      0.96      1047\n",
            "           1       0.95      0.87      0.91       453\n",
            "\n",
            "    accuracy                           0.95      1500\n",
            "   macro avg       0.95      0.92      0.94      1500\n",
            "weighted avg       0.95      0.95      0.95      1500\n",
            "\n",
            "ROC-AUC Score: 0.972744580858587\n"
          ]
        }
      ]
    }
  ]
}