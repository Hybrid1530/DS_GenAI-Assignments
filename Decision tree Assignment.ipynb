{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN1tLYEYefMBWJoFb/wpX9Z"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**Question 1: What is a Decision Tree, and how does it work in the context of classification?**\n","\n","->A Decision Tree is a supervised machine learning algorithm used for classification and also regression tasks. It works by recursively splitting the dataset into smaller, more homogeneous groups based on features, forming a tree-like structure of decision nodes and leaf nodes.\n","\n","How it works in classification:\n","1. Root Node: The tree starts with the root node, representing the entire dataset.\n","2. Splitting: At each node, the algorithm chooses the feature and threshold that best separates the classes.Criteria: Gini Impurity, Entropy (Information Gain).\n","3. Decision Nodes: Internal nodes represent conditions on features .\n","4. Leaf Nodes: Endpoints that represent the predicted class.\n","5. Recursive Partitioning: This process repeats until a stopping condition is met.\n","6. Prediction: For a new instance, the tree is traversed from root to leaf based on feature values, leading to a final class label.\n"],"metadata":{"id":"49Q4-6ODNLU6"}},{"cell_type":"markdown","source":["**Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?**\n","\n","->*Gini Impurity*\n","\n","Definition: Measures the probability that a randomly chosen sample in a node would be incorrectly classified if it were labeled according to the class distribution in that node.\n","\n","*Entropy (Information Gain)*\n","\n","Definition: Measures the level of uncertainty or disorder in the node. Derived from information theory.\n","\n","*Impact on Decision Tree Splits:*\n","\n","* Goal: At each split, the Decision Tree chooses the feature and threshold that reduce impurity the most.\n","1. With Gini: The algorithm picks the split with the lowest Gini impurity after the split.\n","2. With Entropy: The algorithm picks the split with the highest Information Gain (reduction in entropy).\n","Practical Note: Both usually produce similar trees, but Gini is slightly faster to compute."],"metadata":{"id":"AYQSIaIyNwPg"}},{"cell_type":"markdown","source":["**Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.**\n","\n","->*Pre-Pruning:*\n","\n","1. Stops the tree from growing beyond a certain limit during the building process.\n","2. Applied while building the tree.\n","3. Prevent overfitting early by restricting complexity.\n","\n","Advantage-Saves computation time and memory by avoiding unnecessary splits from the start.\n","\n","*Post-Pruning:*\n","\n","1. Grows the full tree first, then removes branches that do not improve performance.\n","2. Applied after the full tree has been built.\n","3. Remove unnecessary complexity after seeing the full tree\n","\n","Advantage-Allows the tree to consider all possible splits before simplifying, which can lead to better accuracy in some cases."],"metadata":{"id":"LUjtUBpvOcel"}},{"cell_type":"markdown","source":["**Question 4: What is Information Gain in Decision Trees, and why is it important for choosing the best split?**\n","\n","->Information Gain (IG) measures the reduction in impurity (uncertainty) in a dataset after splitting it based on a specific feature.It is calculated using Entropy as the impurity measure.\n","\n","It important for choosing the best split because,\n","1. The higher the Information Gain, the more effective the split is at reducing uncertainty.\n","2. Decision Trees choose the feature and threshold with the highest Information Gain because it leads to purer child nodes.\n","3. This process ensures that the tree learns the most informative and discriminative patterns first.\n","\n"],"metadata":{"id":"KJcDv_ybOvRw"}},{"cell_type":"markdown","source":["**Question 5: What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?**\n","\n","->Applications: Used in customer segmentation, medical diagnosis, fraud detection, credit risk assessment, manufacturing quality control, and recommendation systems.\n","\n","Advantages: Easy to interpret, handles numerical & categorical data, no feature scaling needed, works with missing data, models non-linear relationships.\n","\n","Limitations: Prone to overfitting, unstable with small data changes, biased toward majority classes, less precise for continuous outputs, uses greedy splitting that may miss global optimum.\n","\n"],"metadata":{"id":"wOe-mulPPFMA"}},{"cell_type":"code","source":["\"\"\"\n","Question 6: Write a Python program to:\n","● Load the Iris Dataset\n","● Train a Decision Tree Classifier using the Gini criterion\n","● Print the model’s accuracy and feature importances\n","\n","\"\"\"\n","import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","from sklearn.datasets import load_iris\n","# Load the Iris dataset\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","\n","from sklearn.model_selection import train_test_split\n","# Split the dataset into training and testing sets (80% train, 20% test)\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, random_state=42\n",")\n","\n","from sklearn.tree import DecisionTreeClassifier\n","# Initialize the Decision Tree Classifier with Gini criterion\n","clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n","\n","# Train the model\n","clf.fit(X_train, y_train)\n","\n","# Make predictions on the test set\n","y_pred = clf.predict(X_test)\n","from sklearn.metrics import accuracy_score\n","# Calculate accuracy\n","accuracy = accuracy_score(y_test, y_pred)\n","\n","# Print results\n","print(\"Model Accuracy:\", accuracy)\n","print(\"Feature Importances:\", clf.feature_importances_)\n","print(\"Feature Names:\", iris.feature_names)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lzJjqtgyPYmZ","executionInfo":{"status":"ok","timestamp":1754984827564,"user_tz":-330,"elapsed":5142,"user":{"displayName":"Devang Kotkar","userId":"05907746660439424184"}},"outputId":"52f48d28-2fb5-4caa-c852-81f2f04fefd8"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Model Accuracy: 1.0\n","Feature Importances: [0.         0.01667014 0.90614339 0.07718647]\n","Feature Names: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n"]}]},{"cell_type":"code","source":["\"\"\"\n","Question 7: Write a Python program to:\n","● Load the Iris Dataset\n","● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to a fully-grown tree.\n","\n","\"\"\"\n","import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","from sklearn.datasets import load_iris\n","# Load the Iris dataset\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","\n","from sklearn.model_selection import train_test_split\n","# Split the dataset into training and testing sets (80% train, 20% test)\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, random_state=42\n",")\n","\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import accuracy_score\n","# Fully-grown decision tree (no depth limit)\n","full_tree = DecisionTreeClassifier(random_state=42)\n","full_tree.fit(X_train, y_train)\n","y_pred_full = full_tree.predict(X_test)\n","accuracy_full = accuracy_score(y_test, y_pred_full)\n","\n","# Decision tree with max_depth = 3\n","limited_tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n","limited_tree.fit(X_train, y_train)\n","y_pred_limited = limited_tree.predict(X_test)\n","accuracy_limited = accuracy_score(y_test, y_pred_limited)\n","\n","# Print results\n","print(\"Fully-grown Tree Accuracy:\", accuracy_full)\n","print(\"Max Depth = 3 Tree Accuracy:\", accuracy_limited)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"08WkceLXP3Dc","executionInfo":{"status":"ok","timestamp":1754984967180,"user_tz":-330,"elapsed":73,"user":{"displayName":"Devang Kotkar","userId":"05907746660439424184"}},"outputId":"866b488f-09fc-4b53-857c-aaa7c5d6f998"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Fully-grown Tree Accuracy: 1.0\n","Max Depth = 3 Tree Accuracy: 1.0\n"]}]},{"cell_type":"code","source":["\"\"\"\n","Question 8: Write a Python program to:\n","● Load the Boston Housing Dataset\n","● Train a Decision Tree Regressor\n","● Print the Mean Squared Error (MSE) and feature importances\n","\n","\"\"\"\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.metrics import mean_squared_error\n","\n","# Load dataset from URL\n","url = \"https://raw.githubusercontent.com/selva86/datasets/master/BostonHousing.csv\"\n","df = pd.read_csv(url)\n","\n","# Separate features and target\n","X = df.drop(\"medv\", axis=1)  # medv = median value of owner-occupied homes\n","y = df[\"medv\"]\n","\n","# Split dataset into training & testing sets\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, random_state=42\n",")\n","\n","# Train Decision Tree Regressor\n","model = DecisionTreeRegressor(random_state=42)\n","model.fit(X_train, y_train)\n","\n","# Predictions\n","y_pred = model.predict(X_test)\n","\n","# Calculate Mean Squared Error\n","mse = mean_squared_error(y_test, y_pred)\n","print(f\"Mean Squared Error: {mse:.2f}\")\n","\n","# Feature Importances\n","feature_importances = pd.DataFrame({\n","    \"Feature\": X.columns,\n","    \"Importance\": model.feature_importances_\n","}).sort_values(by=\"Importance\", ascending=False)\n","\n","print(\"\\nFeature Importances:\")\n","print(feature_importances)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VBi1KY_DQb7A","executionInfo":{"status":"ok","timestamp":1754985049420,"user_tz":-330,"elapsed":418,"user":{"displayName":"Devang Kotkar","userId":"05907746660439424184"}},"outputId":"a61adaf1-b9ad-44c8-8a25-4e1a68e43dc5"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mean Squared Error: 10.42\n","\n","Feature Importances:\n","    Feature  Importance\n","5        rm    0.600326\n","12    lstat    0.193328\n","7       dis    0.070688\n","0      crim    0.051296\n","4       nox    0.027148\n","6       age    0.013617\n","9       tax    0.012464\n","10  ptratio    0.011012\n","11        b    0.009009\n","2     indus    0.005816\n","1        zn    0.003353\n","8       rad    0.001941\n","3      chas    0.000002\n"]}]},{"cell_type":"code","source":["\"\"\"\n","Question 9: Write a Python program to:\n","● Load the Iris Dataset\n","● Tune the Decision Tree’s max_depth and min_samples_split using GridSearchCV\n","● Print the best parameters and the resulting model accuracy\n","\n","\"\"\"\n","import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","from sklearn.datasets import load_iris\n","# Load the Iris dataset\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","# Split the dataset into training and testing sets (80% train, 20% test)\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, random_state=42\n",")\n","\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import accuracy_score\n","# Define parameter grid for tuning\n","param_grid = {\n","    'max_depth': [2, 3, 4, 5, None],\n","    'min_samples_split': [2, 3, 4, 5]\n","}\n","\n","# Initialize the Decision Tree Classifier\n","dt = DecisionTreeClassifier(random_state=42)\n","\n","# Set up GridSearchCV\n","grid_search = GridSearchCV(\n","    estimator=dt,\n","    param_grid=param_grid,\n","    cv=5,               # 5-fold cross-validation\n","    scoring='accuracy',\n","    n_jobs=-1\n",")\n","\n","# Fit GridSearchCV\n","grid_search.fit(X_train, y_train)\n","\n","# Best parameters\n","best_params = grid_search.best_params_\n","\n","# Evaluate the best model on test set\n","best_model = grid_search.best_estimator_\n","y_pred = best_model.predict(X_test)\n","accuracy = accuracy_score(y_test, y_pred)\n","\n","# Print results\n","print(\"Best Parameters:\", best_params)\n","print(\"Model Accuracy with Best Parameters:\", accuracy)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w5VwIpIjQyQa","executionInfo":{"status":"ok","timestamp":1754985146374,"user_tz":-330,"elapsed":4295,"user":{"displayName":"Devang Kotkar","userId":"05907746660439424184"}},"outputId":"8c77154d-c4a2-4840-8d57-42f824c41b6e"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Best Parameters: {'max_depth': 4, 'min_samples_split': 2}\n","Model Accuracy with Best Parameters: 1.0\n"]}]},{"cell_type":"markdown","source":["**Question 10: Imagine you’re working as a data scientist for a healthcare company that wants to predict whether a patient has a certain disease. You have a large dataset with mixed data types and some missing values.**\n","\n","Explain the step-by-step process you would follow to:\n","* Handle the missing values\n","* Encode the categorical features\n","* Train a Decision Tree model\n","* Tune its hyperparameters\n","* Evaluate its performance And describe what business value this model could provide in the real-world setting\n","\n","->\n","1. Handle Missing Values\n","* Numerical Features: Replace missing values with median or mean (median is safer for skewed data).\n","* Categorical Features: Replace missing values with the most frequent category or use a special “Unknown” label.\n","* Advanced Option: Use SimpleImputer or KNNImputer from sklearn for automated imputation.\n","\n","2. Encode Categorical Features\n","* Nominal Features: Use One-Hot Encoding (pd.get_dummies() or OneHotEncoder).\n","* Ordinal Features: Use Label Encoding or map categories to integers based on order.\n","* Ensure the encoding is consistent for training and future prediction data.\n","\n","3. Train a Decision Tree Model\n","* Split the dataset into training and testing sets (e.g., 80%-20%).\n","* Use DecisionTreeClassifier from sklearn with an initial set of parameters (e.g., criterion='gini').\n","\n","4. Tune Hyperparameters\n","* Use GridSearchCV or RandomizedSearchCV to find optimal max_depth, min_samples_split, and min_samples_leaf.\n","* Apply cross-validation (e.g., 5-fold) to avoid overfitting and get a robust estimate of performance.\n","\n","5. Evaluate Performance\n","* For classification: Check accuracy, precision, recall, F1-score, and ROC-AUC.\n","* Use a confusion matrix to understand class-level performance.\n","* If dataset is imbalanced, focus on recall or F1-score for the disease-positive class.\n","\n","6. Business Value in Real-World Setting:\n","* Faster Diagnosis: Provides doctors with quick and consistent decision support.\n","* Early Detection: Identifies high-risk patients earlier, enabling timely intervention.\n","* Resource Optimization: Helps hospitals prioritize patients who need urgent tests or treatment.\n","* Cost Reduction: Reduces unnecessary diagnostic tests for low-risk patients.\n","* Scalability: Can be applied to large patient datasets across multiple hospitals.\n"],"metadata":{"id":"GCduLCk8RJcs"}}]}