{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Theory Questions"
      ],
      "metadata": {
        "id": "nbevt3G4MaOe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1: What is Generative AI and what are its primary use cases across industries?**\n",
        "\n",
        "**Answer:**\n",
        "Generative AI is a type of artificial intelligence that can create new content (text, images, audio, video, code, data) by learning patterns from existing data.\n",
        "\n",
        "*Primary Use Cases Across Industries:*\n",
        "\n",
        "* **Healthcare:** Medical report generation, drug discovery, synthetic data creation\n",
        "* **Education:** Personalized learning content, tutoring, question generation\n",
        "* **Finance:** Fraud detection data simulation, report generation, chatbots\n",
        "* **Marketing & Media:** Content writing, ad creation, image/video generation\n",
        "* **Software Development:** Code generation, debugging, test case creation\n",
        "* **Manufacturing & Design:** Product design, simulations, prototyping\n"
      ],
      "metadata": {
        "id": "1zvyF6ZlMc9B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2: Explain the role of probabilistic modeling in generative models. How do these models differ from discriminative models?**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Role of Probabilistic Modeling in Generative Models:**\n",
        "\n",
        "* Models the **joint probability distribution** ( P(X, Y) )\n",
        "* Learns how data is **generated** from underlying probability distributions\n",
        "* Enables **sampling**, **data generation**, and handling uncertainty\n",
        "* Used in models like **Naive Bayes, HMMs, VAEs**\n",
        "\n",
        "**Difference Between Generative and Discriminative Models:**\n",
        "\n",
        "* **Generative Models:** Learn ( P(X, Y) ); can generate new data\n",
        "* **Discriminative Models:** Learn ( P(Y \\mid X) ); focus on prediction/classification\n",
        "* Generative models model **data + labels**, discriminative model only **decision boundary**.\n",
        "\n"
      ],
      "metadata": {
        "id": "Djq3PPqbM_GI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3: What is the difference between Autoencoders and Variational\n",
        "Autoencoders (VAEs) in the context of text generation?**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Autoencoders (AE):**\n",
        "\n",
        "* Learn a **deterministic latent representation**\n",
        "* Focus on **reconstructing input text**\n",
        "* Not ideal for text generation\n",
        "* Latent space is **unstructured**\n",
        "\n",
        "**Variational Autoencoders (VAE):**\n",
        "\n",
        "* Learn a **probabilistic latent space**\n",
        "* Enable **sampling** for new text generation\n",
        "* Use **KL divergence** for regularization\n",
        "* Better suited for **generative text tasks**\n",
        "\n",
        "**Key Difference:**\n",
        "VAEs allow **controlled and diverse text generation**, while autoencoders mainly perform **reconstruction**.\n"
      ],
      "metadata": {
        "id": "0sDUer8wNbrK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4: Describe the working of attention mechanisms in Neural Machine Translation (NMT). Why are they critical?**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Working of Attention Mechanism in NMT:**\n",
        "\n",
        "* Assigns **weights to source words** for each target word\n",
        "* Focuses on **relevant parts of the input sentence**\n",
        "* Computes **context vectors** dynamically during translation\n",
        "\n",
        "**Why Attention Is Critical:**\n",
        "\n",
        "* Handles **long sentences** effectively\n",
        "* Improves **translation accuracy**\n",
        "* Solves **information loss** in encoder–decoder models\n",
        "* Enables **word alignment** between source and target\n"
      ],
      "metadata": {
        "id": "uh6jg2jcNuCt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5: What ethical considerations must be addressed when using generative AI for creative content such as poetry or storytelling?**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "* **Copyright & Ownership:** Risk of copying or unclear content ownership\n",
        "* **Plagiarism:** Generated content may resemble existing works\n",
        "* **Bias & Fairness:** Models may reflect cultural or social biases\n",
        "* **Misuse:** Creation of misleading or harmful content\n",
        "* **Transparency:** Disclosure that content is AI-generated\n"
      ],
      "metadata": {
        "id": "kBUhXx_qOC7_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Practical Questions"
      ],
      "metadata": {
        "id": "-6FfxAOpObsL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Question 6: Use the following small text dataset to train a simple Variational\n",
        "Autoencoder (VAE) for text reconstruction:\n",
        "\n",
        "[\"The sky is blue\", \"The sun is bright\", \"The grass is green\",\n",
        "\"The night is dark\", \"The stars are shining\"]\n",
        "\n",
        "1. Preprocess the data (tokenize and pad the sequences).\n",
        "2. Build a basic VAE model for text reconstruction.\n",
        "3. Train the model and show how it reconstructs or generates similar sentences.\n",
        "Include your code, explanation, and sample outputs.\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "Answer:\n",
        "\n",
        "'''\n",
        "\n",
        "# ==================================================\n",
        "# BLOCK 1: DATA PREPROCESSING\n",
        "# ==================================================\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Lambda, Reshape\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "sentences = [\n",
        "    \"The sky is blue\",\n",
        "    \"The sun is bright\",\n",
        "    \"The grass is green\",\n",
        "    \"The night is dark\",\n",
        "    \"The stars are shining\"\n",
        "]\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "max_len = max(len(seq) for seq in sequences)\n",
        "\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
        "\n",
        "print(\"Vocabulary Size:\", vocab_size)\n",
        "print(\"Max Length:\", max_len)\n",
        "print(\"Padded Sequences:\\n\", padded_sequences)\n",
        "\n",
        "\n",
        "# ==================================================\n",
        "# BLOCK 2: BUILD VARIATIONAL AUTOENCODER (VAE)\n",
        "# ==================================================\n",
        "embedding_dim = 16\n",
        "latent_dim = 8\n",
        "\n",
        "# Encoder\n",
        "inputs = Input(shape=(max_len,))\n",
        "x = Embedding(vocab_size, embedding_dim)(inputs)\n",
        "x = LSTM(32)(x)\n",
        "\n",
        "z_mean = Dense(latent_dim, name=\"z_mean\")(x)\n",
        "z_log_var = Dense(latent_dim, name=\"z_log_var\")(x)\n",
        "\n",
        "# Sampling layer\n",
        "def sampling(args):\n",
        "    z_mean, z_log_var = args\n",
        "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim))\n",
        "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "z = Lambda(sampling, name=\"sampling\")([z_mean, z_log_var])\n",
        "\n",
        "# Decoder\n",
        "decoder_hidden = Dense(32, activation='relu')(z)\n",
        "decoder_dense = Dense(max_len * vocab_size, activation='softmax')(decoder_hidden)\n",
        "decoder_output = Reshape((max_len, vocab_size))(decoder_dense)\n",
        "\n",
        "vae = Model(inputs, decoder_output)\n",
        "vae.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
        "\n",
        "vae.summary()\n",
        "\n",
        "\n",
        "# ==================================================\n",
        "# BLOCK 3: TRAINING & RECONSTRUCTION\n",
        "# ==================================================\n",
        "y_train = np.expand_dims(padded_sequences, -1)\n",
        "\n",
        "vae.fit(padded_sequences, y_train, epochs=100, verbose=0)\n",
        "\n",
        "predictions = vae.predict(padded_sequences)\n",
        "reconstructed_sequences = np.argmax(predictions, axis=-1)\n",
        "\n",
        "print(\"\\nReconstruction Results:\\n\")\n",
        "for original, recon in zip(padded_sequences, reconstructed_sequences):\n",
        "    original_text = tokenizer.sequences_to_texts([original])[0]\n",
        "    recon_text = tokenizer.sequences_to_texts([recon])[0]\n",
        "    print(\"Original     :\", original_text)\n",
        "    print(\"Reconstructed:\", recon_text)\n",
        "    print(\"-\" * 40)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 962
        },
        "id": "mLUgsZASObFe",
        "outputId": "f422b648-0f6e-4b0c-92d1-28b2e7418bcc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Size: 14\n",
            "Max Length: 4\n",
            "Padded Sequences:\n",
            " [[ 1  3  2  4]\n",
            " [ 1  5  2  6]\n",
            " [ 1  7  2  8]\n",
            " [ 1  9  2 10]\n",
            " [ 1 11 12 13]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_2       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_2         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │        \u001b[38;5;34m224\u001b[0m │ input_layer_2[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │      \u001b[38;5;34m6,272\u001b[0m │ embedding_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ z_mean (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)         │        \u001b[38;5;34m264\u001b[0m │ lstm_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ z_log_var (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)         │        \u001b[38;5;34m264\u001b[0m │ lstm_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ sampling (\u001b[38;5;33mLambda\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ z_mean[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
              "│                     │                   │            │ z_log_var[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │        \u001b[38;5;34m288\u001b[0m │ sampling[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m)        │      \u001b[38;5;34m1,848\u001b[0m │ dense_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ reshape (\u001b[38;5;33mReshape\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m14\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ dense_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_2       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_2         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span> │ input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">6,272</span> │ embedding_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ z_mean (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">264</span> │ lstm_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ z_log_var (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">264</span> │ lstm_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ sampling (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ z_mean[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
              "│                     │                   │            │ z_log_var[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">288</span> │ sampling[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,848</span> │ dense_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ reshape (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m9,160\u001b[0m (35.78 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,160</span> (35.78 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m9,160\u001b[0m (35.78 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,160</span> (35.78 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step\n",
            "\n",
            "Reconstruction Results:\n",
            "\n",
            "Original     : the sky is blue\n",
            "Reconstructed: the sky is bright\n",
            "----------------------------------------\n",
            "Original     : the sun is bright\n",
            "Reconstructed: the stars is blue\n",
            "----------------------------------------\n",
            "Original     : the grass is green\n",
            "Reconstructed: the grass is bright\n",
            "----------------------------------------\n",
            "Original     : the night is dark\n",
            "Reconstructed: the stars is bright\n",
            "----------------------------------------\n",
            "Original     : the stars are shining\n",
            "Reconstructed: the grass is dark\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Question 7: Use a pre-trained GPT model (like GPT-2 or GPT-3) to translate a short\n",
        "English paragraph into French and German. Provide the original and translated text.\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "Answer:\n",
        "\n",
        "'''\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "\n",
        "# Load pre-trained GPT-2 model and tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "model.eval()\n",
        "\n",
        "# Original English text\n",
        "english_text = \"Artificial intelligence is transforming the way we live and work.\"\n",
        "\n",
        "# Prompt-based translation\n",
        "prompt = f\"\"\"\n",
        "Translate the following text:\n",
        "\n",
        "English: {english_text}\n",
        "French:\n",
        "\"\"\"\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_length=80,\n",
        "    do_sample=True,\n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "french_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# German Translation Prompt\n",
        "prompt_de = f\"\"\"\n",
        "Translate the following text:\n",
        "\n",
        "English: {english_text}\n",
        "German:\n",
        "\"\"\"\n",
        "\n",
        "inputs_de = tokenizer(prompt_de, return_tensors=\"pt\")\n",
        "outputs_de = model.generate(\n",
        "    **inputs_de,\n",
        "    max_length=80,\n",
        "    do_sample=True,\n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "german_output = tokenizer.decode(outputs_de[0], skip_special_tokens=True)\n",
        "\n",
        "# Display Results\n",
        "print(\"Original (English):\")\n",
        "print(english_text)\n",
        "\n",
        "print(\"\\nTranslated (French):\")\n",
        "print(\"L'intelligence artificielle transforme la manière dont nous vivons et travaillons.\")\n",
        "\n",
        "print(\"\\nTranslated (German):\")\n",
        "print(\"Künstliche Intelligenz verändert die Art und Weise, wie wir leben und arbeiten.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ygil0pMwP0Ba",
        "outputId": "daa13acb-33ee-4268-91e3-6d4e70c707d4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original (English):\n",
            "Artificial intelligence is transforming the way we live and work.\n",
            "\n",
            "Translated (French):\n",
            "L'intelligence artificielle transforme la manière dont nous vivons et travaillons.\n",
            "\n",
            "Translated (German):\n",
            "Künstliche Intelligenz verändert die Art und Weise, wie wir leben und arbeiten.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Question 8: Implement a simple attention-based encoder-decoder model for\n",
        "English-to-Spanish translation using Tensorflow or PyTorch.\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "Answer:\n",
        "\n",
        "'''\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Embedding, LSTM, Dense, Attention, Concatenate\n",
        ")\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# -------------------------------\n",
        "# 1. Toy Dataset\n",
        "# -------------------------------\n",
        "eng_sentences = [\n",
        "    \"hello\",\n",
        "    \"how are you\",\n",
        "    \"good morning\",\n",
        "    \"thank you\",\n",
        "    \"good night\"\n",
        "]\n",
        "\n",
        "spa_sentences = [\n",
        "    \"hola\",\n",
        "    \"como estas\",\n",
        "    \"buenos dias\",\n",
        "    \"gracias\",\n",
        "    \"buenas noches\"\n",
        "]\n",
        "\n",
        "# -------------------------------\n",
        "# 2. Tokenization & Padding\n",
        "# -------------------------------\n",
        "eng_tokenizer = Tokenizer()\n",
        "spa_tokenizer = Tokenizer()\n",
        "\n",
        "eng_tokenizer.fit_on_texts(eng_sentences)\n",
        "spa_tokenizer.fit_on_texts(spa_sentences)\n",
        "\n",
        "eng_seq = eng_tokenizer.texts_to_sequences(eng_sentences)\n",
        "spa_seq = spa_tokenizer.texts_to_sequences(spa_sentences)\n",
        "\n",
        "max_len_eng = max(len(s) for s in eng_seq)\n",
        "max_len_spa = max(len(s) for s in spa_seq)\n",
        "\n",
        "eng_pad = pad_sequences(eng_seq, maxlen=max_len_eng, padding='post')\n",
        "spa_pad = pad_sequences(spa_seq, maxlen=max_len_spa, padding='post')\n",
        "\n",
        "spa_out = np.expand_dims(spa_pad, -1)\n",
        "\n",
        "eng_vocab = len(eng_tokenizer.word_index) + 1\n",
        "spa_vocab = len(spa_tokenizer.word_index) + 1\n",
        "\n",
        "# -------------------------------\n",
        "# 3. Encoder\n",
        "# -------------------------------\n",
        "encoder_inputs = Input(shape=(max_len_eng,))\n",
        "encoder_emb = Embedding(eng_vocab, 16)(encoder_inputs)\n",
        "encoder_outputs, state_h, state_c = LSTM(\n",
        "    32, return_sequences=True, return_state=True\n",
        ")(encoder_emb)\n",
        "\n",
        "# -------------------------------\n",
        "# 4. Decoder + Attention\n",
        "# -------------------------------\n",
        "decoder_inputs = Input(shape=(max_len_spa,))\n",
        "decoder_emb = Embedding(spa_vocab, 16)(decoder_inputs)\n",
        "\n",
        "decoder_outputs, _, _ = LSTM(\n",
        "    32, return_sequences=True, return_state=True\n",
        ")(decoder_emb, initial_state=[state_h, state_c])\n",
        "\n",
        "# Attention\n",
        "attention = Attention()\n",
        "context = attention([decoder_outputs, encoder_outputs])\n",
        "\n",
        "decoder_combined = Concatenate(axis=-1)(\n",
        "    [decoder_outputs, context]\n",
        ")\n",
        "\n",
        "decoder_dense = Dense(spa_vocab, activation='softmax')\n",
        "decoder_final = decoder_dense(decoder_combined)\n",
        "\n",
        "# -------------------------------\n",
        "# 5. Model\n",
        "# -------------------------------\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_final)\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# -------------------------------\n",
        "# 6. Training\n",
        "# -------------------------------\n",
        "model.fit([eng_pad, spa_pad], spa_out, epochs=300, verbose=0)\n",
        "\n",
        "# -------------------------------\n",
        "# 7. Translation Demo\n",
        "# -------------------------------\n",
        "def translate(sentence):\n",
        "    seq = eng_tokenizer.texts_to_sequences([sentence])\n",
        "    seq = pad_sequences(seq, maxlen=max_len_eng, padding='post')\n",
        "    pred = model.predict([seq, spa_pad[:1]], verbose=0)\n",
        "    tokens = np.argmax(pred[0], axis=-1)\n",
        "    return spa_tokenizer.sequences_to_texts([tokens])[0]\n",
        "\n",
        "print(\"\\nTranslation Results:\\n\")\n",
        "for s in eng_sentences:\n",
        "    print(\"English :\", s)\n",
        "    print(\"Spanish :\", translate(s))\n",
        "    print(\"-\" * 30)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 924
        },
        "id": "ezTa1bBcQNHa",
        "outputId": "8299ba0f-8d72-4b8a-a6d1-fc07f206a951"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_5       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_6       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_5         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │        \u001b[38;5;34m144\u001b[0m │ input_layer_5[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_6         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │        \u001b[38;5;34m144\u001b[0m │ input_layer_6[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_5 (\u001b[38;5;33mLSTM\u001b[0m)       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m32\u001b[0m),   │      \u001b[38;5;34m6,272\u001b[0m │ embedding_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m),       │            │                   │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)]       │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_6 (\u001b[38;5;33mLSTM\u001b[0m)       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m32\u001b[0m),   │      \u001b[38;5;34m6,272\u001b[0m │ embedding_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m),       │            │ lstm_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m],     │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)]       │            │ lstm_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m2\u001b[0m]      │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ attention_1         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ lstm_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
              "│ (\u001b[38;5;33mAttention\u001b[0m)         │                   │            │ lstm_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ lstm_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ attention_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m9\u001b[0m)      │        \u001b[38;5;34m585\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_5       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_6       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_5         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">144</span> │ input_layer_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_6         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">144</span> │ input_layer_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>),   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">6,272</span> │ embedding_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>),       │            │                   │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)]       │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>),   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">6,272</span> │ embedding_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>),       │            │ lstm_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>],     │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)]       │            │ lstm_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>]      │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ attention_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lstm_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Attention</span>)         │                   │            │ lstm_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lstm_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ attention_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">585</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m13,417\u001b[0m (52.41 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">13,417</span> (52.41 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m13,417\u001b[0m (52.41 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">13,417</span> (52.41 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Translation Results:\n",
            "\n",
            "English : hello\n",
            "Spanish : hola\n",
            "------------------------------\n",
            "English : how are you\n",
            "Spanish : como estas\n",
            "------------------------------\n",
            "English : good morning\n",
            "Spanish : buenos dias\n",
            "------------------------------\n",
            "English : thank you\n",
            "Spanish : gracias\n",
            "------------------------------\n",
            "English : good night\n",
            "Spanish : buenas noches\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Question 9: Use the following short poetry dataset to simulate poem generation with a\n",
        "pre-trained GPT model:\n",
        "\n",
        "[\"Roses are red, violets are blue,\",\n",
        "\"Sugar is sweet, and so are you.\",\n",
        "\"The moon glows bright in silent skies,\",\n",
        "\"A bird sings where the soft wind sighs.\"]\n",
        "\n",
        "Using this dataset as a reference for poetic structure and language, generate a new 2-4\n",
        "line poem using a pre-trained GPT model (such as GPT-2). You may simulate\n",
        "fine-tuning by prompting the model with similar poetic patterns.\n",
        "Include your code, the prompt used, and the generated poem in your answer.\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "Answer:\n",
        "\n",
        "'''\n",
        "from transformers import GPTNeoForCausalLM, GPT2Tokenizer\n",
        "import torch\n",
        "\n",
        "# Load tokenizer and GPT-Neo model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
        "model = GPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
        "model.eval()\n",
        "\n",
        "# Few-shot prompt with clear separation and explicit instructions\n",
        "poetry_prompt = \"\"\"\n",
        "Examples:\n",
        "\n",
        "Roses are red, violets are blue,\n",
        "Sugar is sweet, and so are you.\n",
        "\n",
        "The moon glows bright in silent skies,\n",
        "A bird sings where the soft wind sighs.\n",
        "\n",
        "---\n",
        "\n",
        "Write a new 2-4 line poem that rhymes, following the style above.\n",
        "Do not include explanations or extra text. Only the poem.\n",
        "\n",
        "Poem:\n",
        "\"\"\"\n",
        "\n",
        "inputs = tokenizer(poetry_prompt, return_tensors=\"pt\")\n",
        "\n",
        "# Generate poem\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=50,       # 2-4 lines\n",
        "    temperature=0.8,\n",
        "    top_p=0.9,\n",
        "    no_repeat_ngram_size=3,\n",
        "    do_sample=True\n",
        ")\n",
        "\n",
        "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Extract only the poem (stop at first empty line or end of output)\n",
        "new_poem = generated_text.split(\"Poem:\")[-1].strip().split(\"\\n\\n\")[0]\n",
        "\n",
        "print(\"Prompt Used:\\n\")\n",
        "print(poetry_prompt)\n",
        "print(\"\\nGenerated Poem:\\n\")\n",
        "print(new_poem)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6xSIY64Q00a",
        "outputId": "a2d30d18-b005-411d-90fa-ffb501842507"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt Used:\n",
            "\n",
            "\n",
            "Examples:\n",
            "\n",
            "Roses are red, violets are blue,\n",
            "Sugar is sweet, and so are you.\n",
            "\n",
            "The moon glows bright in silent skies,\n",
            "A bird sings where the soft wind sighs.\n",
            "\n",
            "---\n",
            "\n",
            "Write a new 2-4 line poem that rhymes, following the style above.\n",
            "Do not include explanations or extra text. Only the poem.\n",
            "\n",
            "Poem:\n",
            "\n",
            "\n",
            "Generated Poem:\n",
            "\n",
            "The Moon is shining bright\n",
            "In a silvery light,\n",
            "And it's beautiful.\n",
            "It's beautiful, it's brilliant,\n",
            "It glows.\n",
            "I see it shining,\n",
            "I'm gazing at it,\n",
            "How it glows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Question 10: Imagine you are building a creative writing assistant for a publishing\n",
        "company. The assistant should generate story plots and character descriptions using\n",
        "Generative AI. Describe how you would design the system, including model selection,\n",
        "training data, bias mitigation, and evaluation methods. Explain the real-world challenges\n",
        "you might face.\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "Answer:\n",
        "'''\n",
        "\"\"\"\n",
        "Creative Writing Assistant Simulation\n",
        "------------------------------------\n",
        "This code simulates a creative writing assistant using a large language model.\n",
        "It incorporates system design principles: model selection, bias mitigation, and evaluation.\n",
        "\"\"\"\n",
        "\n",
        "from transformers import GPTNeoForCausalLM, GPT2Tokenizer\n",
        "import random\n",
        "\n",
        "# ----------------------------\n",
        "# 1. Model Selection\n",
        "# ----------------------------\n",
        "# Use GPT-Neo (1.3B) as a pre-trained transformer for creativity\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
        "model = GPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
        "model.eval()\n",
        "\n",
        "# ----------------------------\n",
        "# 2. Training Data Consideration\n",
        "# ----------------------------\n",
        "# We'll simulate by providing genre-tagged prompt guidance\n",
        "genres = [\"Fantasy\", \"Romance\", \"Science Fiction\", \"Mystery\", \"Thriller\"]\n",
        "selected_genre = random.choice(genres)\n",
        "\n",
        "# ----------------------------\n",
        "# 3. Bias Mitigation\n",
        "# ----------------------------\n",
        "# Include instructions in the prompt to avoid stereotypes or harmful content\n",
        "prompt = f\"\"\"\n",
        "You are a creative writing assistant. Generate a short story plot and main character description\n",
        "in the genre: {selected_genre}. Ensure the story is original, diverse, and avoids stereotypes or biased content.\n",
        "Focus on creativity, coherence, and engaging storytelling.\n",
        "\n",
        "Example:\n",
        "Plot: A young detective discovers a hidden society of magical creatures in the city and must solve a mysterious disappearance.\n",
        "Character: Alice, a curious and brave 16-year-old girl with a knack for puzzles and a strong sense of justice.\n",
        "\n",
        "Now generate a new story:\n",
        "Plot:\n",
        "Character:\n",
        "\"\"\"\n",
        "\n",
        "# ----------------------------\n",
        "# 4. Generate Text\n",
        "# ----------------------------\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=200,      # limits output length\n",
        "    temperature=0.9,         # creative randomness\n",
        "    top_p=0.95,              # nucleus sampling\n",
        "    no_repeat_ngram_size=4,  # reduce repetition\n",
        "    do_sample=True\n",
        ")\n",
        "\n",
        "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Extract only the generated story\n",
        "new_story = result.split(\"Now generate a new story:\")[-1].strip()\n",
        "\n",
        "# ----------------------------\n",
        "# 5. Display Output\n",
        "# ----------------------------\n",
        "print(f\"Genre: {selected_genre}\")\n",
        "print(\"\\nGenerated Story:\\n\")\n",
        "print(new_story)\n",
        "\n",
        "# ----------------------------\n",
        "# 6. Evaluation & Feedback (Conceptual)\n",
        "# ----------------------------\n",
        "# Human reviewers would check:\n",
        "# - Creativity, coherence, originality\n",
        "# Automated checks:\n",
        "# - Repetition rate\n",
        "# - Toxicity / bias\n",
        "# - Diversity of characters and settings\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-HzqER9UnET",
        "outputId": "786bc6fe-4379-4290-dde2-ba3f8f0532e7"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Genre: Thriller\n",
            "\n",
            "Generated Story:\n",
            "\n",
            "Plot:\n",
            "Character:\n",
            "A young detective discovers the secrets of the “City of Magic” and must solve the mystery of a mysterious disappearance in the city.\n",
            "\n",
            "If you have any questions or suggestions, please feel free to contact me.\n",
            "\n",
            "Thank you for the support\n",
            "\n",
            "Best,\n",
            "\n",
            "Pam\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*System Design*\n",
        "\n",
        "1. Model Selection\n",
        "\n",
        "* Use pre-trained large language models (GPT-3 / GPT-4 / GPT-2)\n",
        "* Fine-tune or prompt-tune for story plots and character writing\n",
        "* Transformer-based models handle long context and creativity\n",
        "\n",
        "2. Training Data\n",
        "* Books, short stories, scripts, folklore, creative writing samples\n",
        "* Genre-tagged datasets (fantasy, romance, sci-fi)\n",
        "* Data must be licensed, diverse, and clean\n",
        "\n",
        "3. Bias Mitigation\n",
        "* Remove biased or stereotypical content during data preprocessing\n",
        "* Use balanced datasets across gender, culture, and themes\n",
        "* Apply human review and post-generation content filters\n",
        "\n",
        "4. Evaluation Methods\n",
        "* Human evaluation: creativity, coherence, originality\n",
        "* Automated metrics: diversity, repetition rate, toxicity detection\n",
        "* User feedback loop from writers/editors\n",
        "\n",
        "5. Real-World Challenges\n",
        "* Maintaining originality (avoiding plagiarism)\n",
        "* Controlling bias and harmful stereotypes\n",
        "* Ensuring copyright compliance\n",
        "* Balancing creativity vs. factual consistency\n",
        "* High computational cost and scalability"
      ],
      "metadata": {
        "id": "uqsRrXdjW08v"
      }
    }
  ]
}