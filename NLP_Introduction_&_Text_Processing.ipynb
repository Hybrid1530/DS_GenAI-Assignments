{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ivQvVRii1ISR"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Theory Questions"
      ],
      "metadata": {
        "id": "ivQvVRii1ISR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1: What is Computational Linguistics and how does it relate to NLP?**\n",
        "\n",
        "**Answer:** Computational Linguistics is a field that applies computer science techniques to analyze, model, and understand human language. It focuses on creating algorithms and linguistic models that explain how language works.\n",
        "\n",
        "Its relation to Natural Language Processing (NLP) is very close:\n",
        "\n",
        "1. Computational Linguistics provides the theoretical and linguistic foundations (syntax, semantics, morphology, phonology).\n",
        "2. NLP applies these theories to build practical language-processing systems, such as chatbots, translators, speech recognizers, and text analyzers."
      ],
      "metadata": {
        "id": "Bl2NZ80s1NfF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2: Briefly describe the historical evolution of Natural Language Processing.**\n",
        "\n",
        "**Answer:**\n",
        "The evolution of NLP can be summarized in four main phases:\n",
        "\n",
        "1. 1950s–1960s: Rule-Based Systems\n",
        "* NLP began with symbolic, hand-crafted grammar rules and early machine translation experiments.\n",
        "\n",
        "2. 1970s–1980s: Linguistic & Knowledge-Based Approaches\n",
        "* Systems used formal grammars, parsing techniques, and expert knowledge bases to understand language.\n",
        "\n",
        "3. 1990s–2010: Statistical NLP\n",
        "* With large corpora and improved computing power, probabilistic models (HMMs, n-grams, CRFs) and machine learning became dominant.\n",
        "\n",
        "4. 2010–Present: Neural & Deep Learning Era\n",
        "* Neural networks, word embeddings, LSTMs, and later Transformers (e.g., BERT, GPT) revolutionized NLP with high accuracy and end-to-end learning."
      ],
      "metadata": {
        "id": "6EcLlfyi2EU0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3: List and explain three major use cases of NLP in today’s tech industry.**\n",
        "\n",
        "**Answer:** Three major use cases of NLP in today’s tech industry are:\n",
        "\n",
        "1. **Machine Translation :-**\n",
        "Converts text or speech from one language to another (e.g., Google Translate), enabling cross-language communication and global content access.\n",
        "\n",
        "2. **Sentiment Analysis :-**\n",
        "Identifies opinions and emotions in text, widely used in social media monitoring, customer feedback analysis, and brand reputation management.\n",
        "\n",
        "3. **Chatbots & Virtual Assistants :-**\n",
        "Systems like Siri, Alexa, and customer-support bots use NLP to understand user queries and generate meaningful responses, improving automation and user experience.\n"
      ],
      "metadata": {
        "id": "1F_7qnI62g5W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4: What is text normalization and why is it essential in text processing tasks?**\n",
        "\n",
        "**Answer:**\n",
        "* Text normalization is the process of converting text into a consistent, standardized form before analysis.\n",
        "* It includes steps like lowercasing, removing punctuation, expanding contractions, correcting spellings, and lemmatization/stemming.\n",
        "* It is essential because raw text is often messy and inconsistent. Normalization ensures uniformity, reduces noise, and improves the accuracy of downstream NLP tasks such as classification, sentiment analysis, and machine translation."
      ],
      "metadata": {
        "id": "ePXcqd1r3DYE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5: Compare and contrast stemming and lemmatization with suitable\n",
        "examples.**\n",
        "\n",
        "**Answer:**Stemming and lemmatization both reduce words to their base form, but they differ in method and output.\n",
        "\n",
        "**Stemming**\n",
        "* Uses simple cutting rules to remove suffixes.\n",
        "* Often produces non-dictionary words.\n",
        "\n",
        "Example:\n",
        "1. “playing” → play\n",
        "2. “studies” → studi\n",
        "\n",
        "**Lemmatization**\n",
        "* Uses vocabulary and grammatical rules to return the meaningful root form (lemma).\n",
        "* Always produces valid dictionary words.\n",
        "\n",
        "Example:\n",
        "1. “playing” → play\n",
        "2. “studies” → study\n"
      ],
      "metadata": {
        "id": "Ppel6MVa3XeW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Practical Questions"
      ],
      "metadata": {
        "id": "puubr_9G3812"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Question 6: Write a Python program that uses regular expressions (regex) to extract all\n",
        "email addresses from the following block of text:\n",
        "\n",
        "“Hello team, please contact us at support@xyz.com for technical issues, or reach out to\n",
        "our HR at hr@xyz.com. You can also connect with John at john.doe@xyz.org and jenny\n",
        "via jenny_clarke126@mail.co.us. For partnership inquiries, email partners@xyz.biz.”\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "Answer:\n",
        "'''\n",
        "import re\n",
        "\n",
        "text = \"\"\"\n",
        "Hello team, please contact us at support@xyz.com for technical issues,\n",
        "or reach out to our HR at hr@xyz.com. You can also connect with John\n",
        "at john.doe@xyz.org and jenny via jenny_clarke126@mail.co.us.\n",
        "For partnership inquiries, email partners@xyz.biz.\n",
        "\"\"\"\n",
        "\n",
        "# Regex pattern for email extraction\n",
        "pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
        "\n",
        "emails = re.findall(pattern, text)\n",
        "\n",
        "print(emails)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOvM9NcD4AjF",
        "outputId": "e1244d41-0b6c-4d0f-8a3b-6fadd325abe9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['support@xyz.com', 'hr@xyz.com', 'john.doe@xyz.org', 'jenny_clarke126@mail.co.us', 'partners@xyz.biz']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Question 7: Given the sample paragraph below, perform string tokenization and\n",
        "frequency distribution using Python and NLTK:\n",
        "\n",
        "“Natural Language Processing (NLP) is a fascinating field that combines linguistics,\n",
        "computer science, and artificial intelligence. It enables machines to understand,\n",
        "interpret, and generate human language. Applications of NLP include chatbots,\n",
        "sentiment analysis, and machine translation. As technology advances, the role of NLP\n",
        "in modern solutions is becoming increasingly critical.”\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "Answer:\n",
        "\n",
        "'''\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "text = \"\"\"\n",
        "Natural Language Processing (NLP) is a fascinating field that combines linguistics,\n",
        "computer science, and artificial intelligence. It enables machines to understand,\n",
        "interpret, and generate human language. Applications of NLP include chatbots,\n",
        "sentiment analysis, and machine translation. As technology advances, the role of NLP\n",
        "in modern solutions is becoming increasingly critical.\n",
        "\"\"\"\n",
        "\n",
        "# Tokenization\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Stopword Removal\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "filtered_tokens = [word for word in tokens if word.lower() not in stop_words and word.isalpha()]\n",
        "\n",
        "# Frequency Distribution\n",
        "freq_dist = FreqDist(filtered_tokens)\n",
        "\n",
        "print(\"Filtered Tokens:\")\n",
        "print(filtered_tokens)\n",
        "\n",
        "print(\"\\nFrequency Distribution:\")\n",
        "for word, count in freq_dist.items():\n",
        "    print(f\"{word:15} : {count}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOP6zokl4awc",
        "outputId": "3a74cd19-39bf-409f-b2d9-b0639bc17501"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered Tokens:\n",
            "['Natural', 'Language', 'Processing', 'NLP', 'fascinating', 'field', 'combines', 'linguistics', 'computer', 'science', 'artificial', 'intelligence', 'enables', 'machines', 'understand', 'interpret', 'generate', 'human', 'language', 'Applications', 'NLP', 'include', 'chatbots', 'sentiment', 'analysis', 'machine', 'translation', 'technology', 'advances', 'role', 'NLP', 'modern', 'solutions', 'becoming', 'increasingly', 'critical']\n",
            "\n",
            "Frequency Distribution:\n",
            "Natural         : 1\n",
            "Language        : 1\n",
            "Processing      : 1\n",
            "NLP             : 3\n",
            "fascinating     : 1\n",
            "field           : 1\n",
            "combines        : 1\n",
            "linguistics     : 1\n",
            "computer        : 1\n",
            "science         : 1\n",
            "artificial      : 1\n",
            "intelligence    : 1\n",
            "enables         : 1\n",
            "machines        : 1\n",
            "understand      : 1\n",
            "interpret       : 1\n",
            "generate        : 1\n",
            "human           : 1\n",
            "language        : 1\n",
            "Applications    : 1\n",
            "include         : 1\n",
            "chatbots        : 1\n",
            "sentiment       : 1\n",
            "analysis        : 1\n",
            "machine         : 1\n",
            "translation     : 1\n",
            "technology      : 1\n",
            "advances        : 1\n",
            "role            : 1\n",
            "modern          : 1\n",
            "solutions       : 1\n",
            "becoming        : 1\n",
            "increasingly    : 1\n",
            "critical        : 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Question 8: Create a custom annotator using spaCy or NLTK that identifies and labels\n",
        "proper nouns in a given text.\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "Answer:\n",
        "'''\n",
        "import spacy\n",
        "\n",
        "# Load spaCy English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"\"\"\n",
        "Amazon is planning to open a new office in Mumbai.\n",
        "Jeff Bezos visited India last year to discuss partnerships with Reliance Industries.\n",
        "\"\"\"\n",
        "\n",
        "# Process text\n",
        "doc = nlp(text)\n",
        "\n",
        "# Custom annotator: extract and label proper nouns\n",
        "proper_nouns = [(token.text, \"PROPER_NOUN\") for token in doc if token.pos_ == \"PROPN\"]\n",
        "\n",
        "print(\"Proper Noun Annotations:\")\n",
        "for word, label in proper_nouns:\n",
        "    print(f\"{word} --> {label}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRCJ5y625Q8G",
        "outputId": "ab41613a-e67f-43c9-c65e-68aab13387d9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Proper Noun Annotations:\n",
            "Amazon --> PROPER_NOUN\n",
            "Mumbai --> PROPER_NOUN\n",
            "Jeff --> PROPER_NOUN\n",
            "Bezos --> PROPER_NOUN\n",
            "India --> PROPER_NOUN\n",
            "Reliance --> PROPER_NOUN\n",
            "Industries --> PROPER_NOUN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Question 9: Using Genism, demonstrate how to train a simple Word2Vec model on the\n",
        "following dataset consisting of example sentences:\n",
        "\n",
        "dataset = [\n",
        " \"Natural language processing enables computers to understand human language\",\n",
        " \"Word embeddings are a type of word representation that allows words with similar\n",
        "meaning to have similar representation\",\n",
        " \"Word2Vec is a popular word embedding technique used in many NLP applications\",\n",
        " \"Text preprocessing is a critical step before training word embeddings\",\n",
        " \"Tokenization and normalization help clean raw text for modeling\"\n",
        "]\n",
        "\n",
        "Write code that tokenizes the dataset, preprocesses it, and trains a Word2Vec model using\n",
        "Gensim.\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "Answer:\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "import re\n",
        "\n",
        "# Dataset\n",
        "dataset = [\n",
        " \"Natural language processing enables computers to understand human language\",\n",
        " \"Word embeddings are a type of word representation that allows words with similar meaning to have similar representation\",\n",
        " \"Word2Vec is a popular word embedding technique used in many NLP applications\",\n",
        " \"Text preprocessing is a critical step before training word embeddings\",\n",
        " \"Tokenization and normalization help clean raw text for modeling\"\n",
        "]\n",
        "\n",
        "# -------- Preprocessing + Tokenization --------\n",
        "def preprocess(sentence):\n",
        "    sentence = sentence.lower()                         # Lowercase\n",
        "    sentence = re.sub(r'[^a-zA-Z\\s]', '', sentence)    # Remove punctuation\n",
        "    tokens = sentence.split()                          # Tokenization\n",
        "    return tokens\n",
        "\n",
        "processed_data = [preprocess(sent) for sent in dataset]\n",
        "\n",
        "print(\"Tokenized & Preprocessed Sentences:\\n\", processed_data)\n",
        "\n",
        "# -------- Train Word2Vec Model --------\n",
        "model = Word2Vec(\n",
        "    sentences=processed_data,\n",
        "    vector_size=50,     # Dimensionality of vectors\n",
        "    window=5,           # Context window\n",
        "    min_count=1,        # Include all words\n",
        "    workers=2,\n",
        "    sg=1                # Skip-gram model\n",
        ")\n",
        "\n",
        "# -------- Example Output: Similar Words --------\n",
        "print(\"\\nSimilar words to 'language':\")\n",
        "print(model.wv.most_similar(\"language\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTy4-YpT5lV8",
        "outputId": "02081020-528c-4e72-9135-357a4896e67c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized & Preprocessed Sentences:\n",
            " [['natural', 'language', 'processing', 'enables', 'computers', 'to', 'understand', 'human', 'language'], ['word', 'embeddings', 'are', 'a', 'type', 'of', 'word', 'representation', 'that', 'allows', 'words', 'with', 'similar', 'meaning', 'to', 'have', 'similar', 'representation'], ['wordvec', 'is', 'a', 'popular', 'word', 'embedding', 'technique', 'used', 'in', 'many', 'nlp', 'applications'], ['text', 'preprocessing', 'is', 'a', 'critical', 'step', 'before', 'training', 'word', 'embeddings'], ['tokenization', 'and', 'normalization', 'help', 'clean', 'raw', 'text', 'for', 'modeling']]\n",
            "\n",
            "Similar words to 'language':\n",
            "[('used', 0.3150634169578552), ('for', 0.2372531294822693), ('meaning', 0.21844501793384552), ('of', 0.20309294760227203), ('to', 0.1847095489501953), ('and', 0.17878921329975128), ('allows', 0.14185984432697296), ('text', 0.13890081644058228), ('are', 0.13452698290348053), ('preprocessing', 0.11213743686676025)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10: Imagine you are a data scientist at a fintech startup. You’ve been tasked with analyzing customer feedback. Outline the steps you would take to clean, process, and extract useful insights using NLP techniques from thousands of customer reviews.**\n",
        "\n",
        "**Answer:**\n",
        "Steps to Clean, Process & Extract Insights from Customer Reviews\n",
        "\n",
        "1. **Data Collection & Loading**\n",
        "* Import customer reviews from CSV/Database.\n",
        "* Remove duplicates and missing values.\n",
        "\n",
        "2. Text Cleaning\n",
        "* Lowercasing\n",
        "* Removing punctuation, numbers, special characters\n",
        "\n",
        "3. Removing stopwords\n",
        "* Lemmatization for normalization\n",
        "\n",
        "4. Tokenization\n",
        "* Split text into individual words/tokens.\n",
        "\n",
        "5. Exploratory Text Analysis\n",
        "* Word frequency distribution\n",
        "* N-grams (common phrases)\n",
        "* Word clouds or summary statistics\n",
        "\n",
        "6 Sentiment Analysis\n",
        "* Rule-based (VADER) or model-based analysis\n",
        "* Identify positive, negative, and neutral reviews\n",
        "\n",
        "7. Topic Modeling\n",
        "* Use LDA to discover key themes in customer feedback\n",
        "* Helps identify recurring issues/features customers mention\n",
        "\n",
        "8. Insight Extraction & Reporting\n",
        "* Summaries of top complaints, positive highlights\n",
        "* Visual charts to present findings to stakeholders"
      ],
      "metadata": {
        "id": "L0qCvvFn6psd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Question 10: Imagine you are a data scientist at a fintech startup. You’ve been tasked\n",
        "with analyzing customer feedback. Outline the steps you would take to clean, process,\n",
        "and extract useful insights using NLP techniques from thousands of customer reviews.\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "Answer:\n",
        "'''\n",
        "# Install required libraries\n",
        "#!pip install nltk gensim wordcloud\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from nltk.probability import FreqDist\n",
        "from gensim import corpora, models\n",
        "\n",
        "# ---------------- SAMPLE DATA ----------------\n",
        "reviews = [\n",
        "    \"The app is great but the login process is slow.\",\n",
        "    \"Customer support is excellent!\",\n",
        "    \"I faced issues with payment verification.\",\n",
        "    \"Very smooth experience. Loved the UI.\",\n",
        "    \"Payments fail sometimes, please fix this.\",\n",
        "    \"Fast service but needs better security features.\"\n",
        "]\n",
        "\n",
        "df = pd.DataFrame({\"review\": reviews})\n",
        "\n",
        "# ---------------- PREPROCESSING ----------------\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def clean_review(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [lemmatizer.lemmatize(word)\n",
        "              for word in tokens if word not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "df[\"cleaned\"] = df[\"review\"].apply(clean_review)\n",
        "\n",
        "# ---------------- SENTIMENT ANALYSIS ----------------\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "def get_sentiment(txt):\n",
        "    return sia.polarity_scores(txt)\n",
        "\n",
        "df[\"sentiment\"] = df[\"review\"].apply(get_sentiment)\n",
        "\n",
        "# ---------------- WORD FREQUENCY ----------------\n",
        "all_words = [word for tokens in df[\"cleaned\"] for word in tokens]\n",
        "freq = FreqDist(all_words)\n",
        "\n",
        "# ---------------- TOPIC MODELING ----------------\n",
        "dictionary = corpora.Dictionary(df[\"cleaned\"])\n",
        "corpus = [dictionary.doc2bow(text) for text in df[\"cleaned\"]]\n",
        "\n",
        "lda = models.LdaModel(corpus, num_topics=2, id2word=dictionary, passes=10)\n",
        "\n",
        "# ---------------- CLEAN OUTPUT ----------------\n",
        "print(\"CLEANED REVIEWS:\")\n",
        "print(df[\"cleaned\"], \"\\n\")\n",
        "\n",
        "print(\"WORD FREQUENCY (TOP 10):\")\n",
        "print(freq.most_common(10), \"\\n\")\n",
        "\n",
        "print(\"SENTIMENT SCORES:\")\n",
        "print(df[\"sentiment\"], \"\\n\")\n",
        "\n",
        "print(\"TOPIC MODELING (LDA):\")\n",
        "for i, topic in lda.print_topics():\n",
        "    print(f\"Topic {i}: {topic}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qP6EJg236Zf6",
        "outputId": "dad10a64-4d92-45e8-865c-4bbef0bbde14"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLEANED REVIEWS:\n",
            "0                  [app, great, login, process, slow]\n",
            "1                      [customer, support, excellent]\n",
            "2               [faced, issue, payment, verification]\n",
            "3                     [smooth, experience, loved, ui]\n",
            "4             [payment, fail, sometimes, please, fix]\n",
            "5    [fast, service, need, better, security, feature]\n",
            "Name: cleaned, dtype: object \n",
            "\n",
            "WORD FREQUENCY (TOP 10):\n",
            "[('payment', 2), ('app', 1), ('great', 1), ('login', 1), ('process', 1), ('slow', 1), ('customer', 1), ('support', 1), ('excellent', 1), ('faced', 1)] \n",
            "\n",
            "SENTIMENT SCORES:\n",
            "0    {'neg': 0.0, 'neu': 0.779, 'pos': 0.221, 'comp...\n",
            "1    {'neg': 0.0, 'neu': 0.23, 'pos': 0.77, 'compou...\n",
            "2    {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...\n",
            "3    {'neg': 0.0, 'neu': 0.546, 'pos': 0.454, 'comp...\n",
            "4    {'neg': 0.357, 'neu': 0.408, 'pos': 0.235, 'co...\n",
            "5    {'neg': 0.0, 'neu': 0.418, 'pos': 0.582, 'comp...\n",
            "Name: sentiment, dtype: object \n",
            "\n",
            "TOPIC MODELING (LDA):\n",
            "Topic 0: 0.055*\"security\" + 0.055*\"service\" + 0.055*\"need\" + 0.055*\"better\" + 0.055*\"feature\" + 0.055*\"fast\" + 0.055*\"great\" + 0.055*\"process\" + 0.055*\"login\" + 0.055*\"app\"\n",
            "Topic 1: 0.096*\"payment\" + 0.058*\"please\" + 0.058*\"fail\" + 0.058*\"sometimes\" + 0.058*\"fix\" + 0.057*\"faced\" + 0.057*\"issue\" + 0.057*\"verification\" + 0.057*\"experience\" + 0.057*\"ui\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        }
      ]
    }
  ]
}